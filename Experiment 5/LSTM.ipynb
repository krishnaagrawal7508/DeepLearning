{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA6M20tsNs2M"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import unicodedata\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl9m8co0N7ws"
      },
      "outputs": [],
      "source": [
        "# --------- Hyperparameters ---------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 64\n",
        "EMBED_SIZE = 256\n",
        "HIDDEN_SIZE = 512\n",
        "NUM_EPOCHS = 20\n",
        "MAX_LEN = 15\n",
        "TEACHER_FORCING_RATIO = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmphrHfNN_hN"
      },
      "outputs": [],
      "source": [
        "# --------- Data Preprocessing ---------\n",
        "def normalize_text(s):\n",
        "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "    s = re.sub(r\"[^a-zA-Z.!?Â¿]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXpwKWH8OA5d"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess data\n",
        "pairs = []\n",
        "with open('spa.txt', encoding='utf-8') as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "    for line in lines:\n",
        "        eng, spa = line.split('\\t')[:2]\n",
        "        eng, spa = normalize_text(eng), normalize_text(spa)\n",
        "        pairs.append((eng, f\"<sos> {spa} <eos>\"))\n",
        "\n",
        "eng_sentences, spa_sentences = zip(*pairs)\n",
        "train_eng, val_eng, train_spa, val_spa = train_test_split(eng_sentences, spa_sentences, test_size=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th3GohD1OB1j"
      },
      "outputs": [],
      "source": [
        "# Tokenizer\n",
        "from collections import Counter\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    word_counts = Counter()\n",
        "    for sentence in sentences:\n",
        "        word_counts.update(sentence.split())\n",
        "    vocab = ['<pad>', '<unk>'] + sorted(word_counts.keys())\n",
        "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
        "    idx2word = {i: w for w, i in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "src_word2idx, src_idx2word = build_vocab(train_eng)\n",
        "tgt_word2idx, tgt_idx2word = build_vocab(train_spa)\n",
        "SRC_VOCAB_SIZE = len(src_word2idx)\n",
        "TGT_VOCAB_SIZE = len(tgt_word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhN5vHZSOD8Q"
      },
      "outputs": [],
      "source": [
        "def encode(sentence, word2idx):\n",
        "    return [word2idx.get(w, word2idx['<unk>']) for w in sentence.split()[:MAX_LEN]]\n",
        "\n",
        "def pad(seq):\n",
        "    return seq + [0] * (MAX_LEN - len(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz3y3G9XOFM-"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, src, tgt):\n",
        "        self.src = [pad(encode(s, src_word2idx)) for s in src]\n",
        "        self.tgt = [pad(encode(s, tgt_word2idx)) for s in tgt]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.src[idx]), torch.tensor(self.tgt[idx])\n",
        "\n",
        "train_loader = DataLoader(TranslationDataset(train_eng, train_spa), batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOb1XZAFOHgY"
      },
      "outputs": [],
      "source": [
        "# --------- Encoder ---------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(SRC_VOCAB_SIZE, EMBED_SIZE)\n",
        "        self.lstm = nn.LSTM(EMBED_SIZE, HIDDEN_SIZE, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embed(x)\n",
        "        outputs, (h, c) = self.lstm(embedded)\n",
        "        return h, c\n",
        "\n",
        "# --------- Decoder ---------\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(TGT_VOCAB_SIZE, EMBED_SIZE)\n",
        "        self.lstm = nn.LSTM(EMBED_SIZE, HIDDEN_SIZE, batch_first=True)\n",
        "        self.fc = nn.Linear(HIDDEN_SIZE, TGT_VOCAB_SIZE)\n",
        "\n",
        "    def forward(self, x, h, c):\n",
        "        x = x.unsqueeze(1)\n",
        "        embedded = self.embed(x)\n",
        "        output, (h, c) = self.lstm(embedded, (h, c))\n",
        "        return self.fc(output.squeeze(1)), h, c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJSJctPJOIiw"
      },
      "outputs": [],
      "source": [
        "# --------- Seq2Seq ---------\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        batch_size, tgt_len = tgt.shape\n",
        "        outputs = torch.zeros(batch_size, tgt_len, TGT_VOCAB_SIZE).to(device)\n",
        "\n",
        "        h, c = self.encoder(src.to(device))\n",
        "        x = tgt[:, 0].to(device)\n",
        "\n",
        "        for t in range(1, tgt_len):\n",
        "            output, h, c = self.decoder(x, h, c)\n",
        "            outputs[:, t] = output\n",
        "            teacher_force = torch.rand(1).item() < TEACHER_FORCING_RATIO\n",
        "            x = tgt[:, t] if teacher_force else output.argmax(1)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln4zdmA_OJsO",
        "outputId": "5e3018b4-59dd-456b-e912-f629148ab9c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 5.3167\n",
            "Epoch 2, Loss: 3.8150\n",
            "Epoch 3, Loss: 3.0193\n",
            "Epoch 4, Loss: 2.4428\n",
            "Epoch 5, Loss: 2.0017\n",
            "Epoch 6, Loss: 1.6759\n",
            "Epoch 7, Loss: 1.4310\n",
            "Epoch 8, Loss: 1.2421\n",
            "Epoch 9, Loss: 1.0975\n",
            "Epoch 10, Loss: 0.9807\n",
            "Epoch 11, Loss: 0.8792\n",
            "Epoch 12, Loss: 0.7982\n",
            "Epoch 13, Loss: 0.7251\n",
            "Epoch 14, Loss: 0.6597\n",
            "Epoch 15, Loss: 0.6180\n",
            "Epoch 16, Loss: 0.5689\n",
            "Epoch 17, Loss: 0.5336\n",
            "Epoch 18, Loss: 0.4987\n",
            "Epoch 19, Loss: 0.4697\n",
            "Epoch 20, Loss: 0.4441\n"
          ]
        }
      ],
      "source": [
        "# --------- Training ---------\n",
        "model = Seq2Seq().to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        output = model(src, tgt)\n",
        "        output = output[:, 1:].reshape(-1, TGT_VOCAB_SIZE)\n",
        "        tgt = tgt[:, 1:].reshape(-1)\n",
        "        loss = criterion(output, tgt)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfeQRWGqOLFG"
      },
      "outputs": [],
      "source": [
        "# --------- Translation ---------\n",
        "def translate(sentence):\n",
        "    model.eval()\n",
        "    src = pad(encode(normalize_text(sentence), src_word2idx))\n",
        "    src_tensor = torch.tensor([src]).to(device)\n",
        "    h, c = model.encoder(src_tensor)\n",
        "\n",
        "    tgt_indices = [tgt_word2idx['<sos>']]\n",
        "    for _ in range(MAX_LEN):\n",
        "        x = torch.tensor([tgt_indices[-1]]).to(device)\n",
        "        output, h, c = model.decoder(x, h, c)\n",
        "        pred = output.argmax(1).item()\n",
        "        if pred == tgt_word2idx['<eos>']:\n",
        "            break\n",
        "        tgt_indices.append(pred)\n",
        "\n",
        "    return ' '.join([tgt_idx2word[i] for i in tgt_indices[1:]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZFRF7-AQqiJ",
        "outputId": "f2316e1b-9e23-45be-9b06-577859deb5d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate: 'Hey! What are you doing?' -> Â¿que Â¿que significa haciendo?\n",
            "Translate: 'Run! There is danger ahead' -> algo alla es bajo el aire.\n"
          ]
        }
      ],
      "source": [
        "# Sample Translations\n",
        "print(\"Translate: 'Hey! What are you doing?'\", \"->\", translate(\"Hey! What are you doing?\"))\n",
        "print(\"Translate: 'Run! There is danger ahead'\", \"->\", translate(\"Run! There is danger ahead\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn5C0lTOQ0IO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
